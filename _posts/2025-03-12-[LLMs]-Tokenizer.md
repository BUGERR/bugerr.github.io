---
title: 'LLMs: Tokenlizer'
date: 2025-03-12
permalink: /posts/2025/03/tokenizer/
tags:
  - LLMs
  - Tokenlizer
  - 
---

# Tokenlizer

[TOC]

## 2. spaCy
### 1. en_core_web_sm
1. func \
English pipeline optimized for CPU. \
Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer. \

2. use \
å®‰è£…ï¼š
```
$ python -m spacy install en_core_web_sm
```

demo:
``` python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for token in doc:
    print(token.text, token.pos_, token.dep_)

'''
Apple is looking at buying U.K. startup for $1 billion
Apple PROPN nsubj
is AUX aux
looking VERB ROOT
at ADP prep
buying VERB pcomp
U.K. PROPN dobj
startup NOUN dep
for ADP prep
$ SYM quantmod
1 NUM compound
billion NUM pobj
'''

```


## 3. huggingface tokenizers
### 1. build tokenizer
1. ç”¨ tokenizers åº“æ—¶ï¼Œæ„å»ºé¡ºåºï¼š
- åˆå§‹åŒ–åˆ†è¯å™¨(BPE)
- å½’ä¸€åŒ–(å¦‚æœcorpusé¢„å¤„ç†å¥½å°±ä¸ç”¨äº†)
- å®šä¹‰é¢„åˆ†è¯æ–¹å¼(Whitespaceï¼Œç¡®ä¿tokenä¸æ˜¯å¤šä¸ªå•è¯)
- åˆå§‹åŒ–è®­ç»ƒå™¨å’Œè¿­ä»£å™¨(å¼•å…¥ç‰¹æ®Štokenï¼Œé¡ºåºå†³å®šäº†idå€¼ã€‚å®šä¹‰vocab_sizeæˆ–min_frequency)
- enable_paddingï¼šbatchå†…è‡ªåŠ¨æŒ‰æœ€é•¿çš„å¥å­è¡¥0
- truncation(max_len)
- åå¤„ç†(ç‰¹æ®Štokenè§„åˆ™)
- ä¿å­˜è¯¥åˆ†è¯å™¨

2. demo: subword BPE tokenizer from Vanilla Transformer \
   æ•°æ®å·²ç»é¢„å¤„ç†å¥½äº†ï¼Œä¸ç”¨å†normalizeã€‚token å’Œå…¶å¯¹åº” index çš„è¯å…¸æ˜¯å·²ç»é›†æˆåœ¨å¯¹åº”è¯­è¨€çš„ tokenizer é‡Œäº†ã€‚
   - batch_iteratorï¼šæ•°æ®é›†æ¯ä¸ªåˆ’åˆ†æ˜¯ä¸€ä¸ª Dataset å¯¹è±¡ã€‚å¤–å±‚ for æŒ‰ batch_size çš„æ­¥é•¿éå†ä½ç½®ï¼Œå¾ªç¯å†…å±‚ç”¨åˆ‡ç‰‡ï¼Œå…ˆæŒ‡å®šæ•°æ®æ¡ç›®èŒƒå›´ï¼Œå†æŒ‡å®šå…·ä½“åˆ—ã€‚è¦çš„æ˜¯è¿™éƒ¨åˆ†åˆ‡ç‰‡çš„translation é¡¹ï¼Œè€Œä¸æ˜¯ translation å±æ€§çš„éƒ¨åˆ†åˆ‡ç‰‡ã€‚
   - è¿”å›é”®å€¼å¯¹å½¢å¼çš„å…ƒç»„ listï¼Œä¾‹å¦‚ [['en':'abc'], ['en:'def']]
``` python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing

SOS_TOKEN = "[SOS]"
EOS_TOKEN = "[EOS]"
PAD_TOKEN = "[PAD]"
UNK_TOKEN = "[UNK]"

special_tokens = [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN]

def build_tokenizer(dataset, lang, force_reload):
    tokenizer_path = config.dataset_dir / f"tokenizer-{lang}.json"
    if os.path.exists(tokenizer_path) and not force_reload:
        tokenizer = Tokenizer(BPE(unk_token=UNK_TOKEN)).from_file(str(tokenizer_path))
    else:
        tokenizer = Tokenizer(BPE(unk_token=UNK_TOKEN))
        tokenizer.pre_tokenizer = Whitespace()
        trainer = BpeTrainer(
            special_tokens=special_tokens, show_progress=True, min_frequency=2
        )
        print(f"Training tokenizer for {lang}...")

        def batch_iterator():
            for i in range(0, dataset["train"].num_rows, config.batch_size):
                batch = dataset["train"][i : i + config.batch_size]["translation"]
                yield [item[lang] for item in batch]

        tokenizer.train_from_iterator(batch_iterator(), trainer)
        tokenizer.enable_padding(
            pad_id=tokenizer.token_to_id(PAD_TOKEN), pad_token=PAD_TOKEN
        )
        tokenizer.enable_truncation(max_length=config.max_len)
        tokenizer.post_processor = TemplateProcessing(
            single=f"{SOS_TOKEN} $A {EOS_TOKEN}",
            pair=f"{SOS_TOKEN} $A {EOS_TOKEN} $B:1 {EOS_TOKEN}:1",  # not used
            special_tokens=[
                (SOS_TOKEN, tokenizer.token_to_id(SOS_TOKEN)),
                (EOS_TOKEN, tokenizer.token_to_id(EOS_TOKEN)),
            ],
        )
        tokenizer.save(str(tokenizer_path))
    return tokenizer

'''
huggingface iwslt2017 An example of 'train' looks as follows.
{
    "translation": {
        "de": "Die nÃ¤chste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.",
        "en": "The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years."
    }
}
'''
```

3. demoï¼šBERT with WordPiece

``` python

from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers import normalizers
from tokenizers.normalizers import NFD, Lowercase, StripAccents
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing
from tokenizers.trainers import WordPieceTrainer

bert_tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])
bert_tokenizer.pre_tokenizer = Whitespace()
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

trainer = WordPieceTrainer(vocab_size=30522, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
bert_tokenizer.train(files, trainer)
bert_tokenizer.save("data/bert-wiki.json")

```

4. Training from memory
- demo_3 æ˜¯ç”¨ text file è®­ç»ƒåˆ†è¯æ¨¡å‹ï¼Œä½†å®é™…ä¸Šå¯ä»¥ç”¨ä»»æ„ Python Iteratior ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚å…ˆåˆå§‹åŒ–ä¸€ä¸ª Unigram tokenizerï¼Œä½¿ç”¨äº† ByteLevel ä½œä¸ºé¢„åˆ†è¯å’Œå¯¹åº”è§£ç ã€‚

``` python 
from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers
tokenizer = Tokenizer(models.Unigram())
tokenizer.normalizer = normalizers.NFKC()
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
tokenizer.decoder = decoders.ByteLevel()
trainer = trainers.UnigramTrainer(
    vocab_size=20000,
    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
    special_tokens=["<PAD>", "<BOS>", "<EOS>"],
)
```
- ç”¨åŸºæœ¬æ•°æ®ç±»å‹ä½œä¸ºè®­ç»ƒæ•°æ®ï¼šListï¼ŒTupleï¼Œnp.Arrayç­‰ä½œä¸ºè¿­ä»£å™¨æ¥æä¾›stringsã€‚

``` python
# First few lines of the "Zen of Python" https://www.python.org/dev/peps/pep-0020/
data = [
    "Beautiful is better than ugly."
    "Explicit is better than implicit."
    "Simple is better than complex."
    "Complex is better than complicated."
    "Flat is better than nested."
    "Sparse is better than dense."
    "Readability counts."
]
tokenizer.train_from_iterator(data, trainer=trainer)
```
- ç”¨ datasets åº“ï¼Œå…ˆæ„é€  dataset çš„è¿­ä»£å™¨ batch_iteratorã€‚ç±»ä¼¼æ„é€  dataloader çš„ collate_fnï¼ŒæŒ‡å®šåŸå§‹æ•°æ®çš„æŸä¸ª column æ¡ç›®ç”¨ä½œè®­ç»ƒã€‚é€šè¿‡ batch è®­ç»ƒåˆ†è¯å™¨ã€‚
- demoï¼šå…ˆæŒ‡å®šäº†åªç”¨ text åˆ—ï¼ŒæŒ‰ batch_size ä½œä¸ºè¿­ä»£æ­¥é•¿ã€‚æœ€åæŒ‡å®šè¿”å›å­—å…¸çš„ç´¢å¼•åä¸º textã€‚

``` python
import datasets
dataset = datasets.load_dataset("wikitext", "wikitext-103-raw-v1", split="train+test+validation")

def batch_iterator(batch_size=1000):
    # Only keep the text column to avoid decoding the rest of the columns unnecessarily
    tok_dataset = dataset.select_columns("text")
    for batch in tok_dataset.iter(batch_size):
        yield batch["text"]

tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))

'''
wikitext:
{
    "text": "\" The Sinclair Scientific Programmable was introduced in 1975 , with the same case as the Sinclair Oxford . It was larger than t..."
}
'''
```

- ç”¨ gzip æ–‡ä»¶ï¼š

``` python
import gzip
with gzip.open("data/my-file.0.gz", "rt") as f:
    tokenizer.train_from_iterator(f, trainer=trainer)

files = ["data/my-file.0.gz", "data/my-file.1.gz", "data/my-file.2.gz"]
def gzip_iterator():
    for path in files:
        with gzip.open(path, "rt") as f:
            for line in f:
                yield line
tokenizer.train_from_iterator(gzip_iterator(), trainer=trainer)
```

### 2. Using the tokenizer
1. encode textè¿”å›ä¸€ä¸ªEncodingå¯¹è±¡ï¼Œå±æ€§åŒ…æ‹¬ï¼š
   - tokens
   - idsï¼štokenåœ¨vocabé‡Œçš„index
   - offsetsï¼šå°†tokenå¯¹åº”å›åŸæ–‡ä¸­çš„ä½ç½®
encode demo: 

``` python
sentence = "Hello, y'all! How are you ğŸ˜ ?"
output = tokenizer.encode(sentence)

print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]

print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]

print(output.offsets[9])
# (26, 27)
sentence[26:27]
# "ğŸ˜"
```

2. encode_batch \
   æ‰¹å¤„ç†çš„æƒ…å†µï¼Œå¯ä»¥ç”¨ enable_padding è®©batchå†…è‡ªåŠ¨æŒ‰æœ€é•¿çš„å¥å­è¡¥0.å¹¶ä¸”è‡ªå¸¦ attention_mask.

demo: encode_batch
``` python
output = tokenizer.encode_batch(["Hello, y'all!", "How are you ğŸ˜ ?"])

# sentences pair çš„æƒ…å†µï¼šlist çš„ list
output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you ğŸ˜ ?"], ["Hello to you too!", "I'm fine, thank you!"]]
)
```
demo: enable_padding
``` python
tokenizer.enable_padding(
    pad_id=tokenizer.token_to_id(PAD_TOKEN), pad_token=PAD_TOKEN
)

output = tokenizer.encode_batch(["Hello, y'all!", "How are you ğŸ˜ ?"])
print(output[1].tokens)
# ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]

print(output[1].attention_mask)
# [1, 1, 1, 1, 1, 1, 1, 0]
```

3. demoï¼šç”¨ IWSLT 2017 dataset è®­ç»ƒè‹±è¯­å’Œå¾·è¯­åˆ†è¯å™¨ï¼Œå¹¶å¯¹æ•°æ®é›†åˆ†è¯ï¼Œç„¶ååˆ¶ä½œè¯­è¨€æ¨¡å‹çš„datasetï¼Œæ¯ä¸ªbatchçš„itemæ˜¯è½¬ä¸ºtensorçš„åˆ†è¯token idã€‚ç„¶åæŠŠsrcå’Œtgtçš„batch paddingåˆ°ä¸€æ ·é•¿ã€‚ï¼ˆä¸ºä»€ä¹ˆè¾“å…¥è¾“å‡ºè¦è¡¥åˆ°ä¸€æ ·é•¿ï¼Ÿä¸ä¸€æ ·ä¹Ÿå¯ä»¥å•Šã€‚æ£€éªŒè¾“å‡ºç»“æœæ˜¯å¦æ­£ç¡®ï¼šæŠŠè¾“å…¥æ•°æ®çš„idè½¬å›textï¼Œå°±èƒ½çœ‹åˆ°è¾“å…¥åŸæ–‡äº†ï¼‰

``` python
def load_data(
    src_lang, tgt_lang, splits: Optional[Sequence[str]] = None, force_reload=False
):
    """
    Load IWSLT 2017 dataset..
    Args:
        src_lang (str): 
            Source language, which depends on which language pair you download.
        tgt_lang (str): 
            Target language, which depends on which language pair you download.
        splits (`Sequence[str]`, *optional*): 
            The splits you want to load. It can be arbitrary combination of "train", "test" and "validation".
            If not speficied, all splits will be loaded.
        force_reload (`bool`, defaults to `False`): 
            If set to `True`, it will re-train a new tokenizer with BPE.
    """
    if sorted((src_lang, tgt_lang)) != ["de", "en"]:
        raise ValueError("Available language options are ('de','en') and ('en', 'de')")
    all_splits = ["train", "validation", "test"]
    if splits is None:
        splits = all_splits
    elif not set(splits).issubset(all_splits):
        raise ValueError(f"Splits should only contain some of {all_splits}")

    dataset = datasets.load_dataset(
        "iwslt2017", f"iwslt2017-{src_lang}-{tgt_lang}", trust_remote_code=True
    )

    src_tokenizer = build_tokenizer(dataset, src_lang, force_reload)
    tgt_tokenizer = build_tokenizer(dataset, tgt_lang, force_reload)

    def collate_fn(batch):
        src_batch, tgt_batch = [], []
        for item in batch:
            src_batch.append(item["translation"][src_lang])
            tgt_batch.append(item["translation"][tgt_lang])

        src_batch = src_tokenizer.encode_batch(src_batch)
        tgt_batch = tgt_tokenizer.encode_batch(tgt_batch)

        src_tensor = torch.LongTensor([item.ids for item in src_batch])
        tgt_tensor = torch.LongTensor([item.ids for item in tgt_batch])

        if src_tensor.shape[-1] < tgt_tensor.shape[-1]:
            src_tensor = F.pad(
                src_tensor,
                [0, tgt_tensor.shape[-1] - src_tensor.shape[-1]],
                value=src_tokenizer.token_to_id(PAD_TOKEN),
            )
        else:
            tgt_tensor = F.pad(
                tgt_tensor,
                [0, src_tensor.shape[-1] - tgt_tensor.shape[-1]],
                value=tgt_tokenizer.token_to_id(PAD_TOKEN),
            )

        return src_tensor, tgt_tensor

    dataloaders = [
        DataLoader(
            dataset[split],
            batch_size=config.batch_size,
            collate_fn=collate_fn,
            shuffle=split == "train",
        )
        for split in splits
    ]

    return (src_tokenizer, tgt_tokenizer, *dataloaders)
```

### 3. pipeline
- Encoding: text --> words --> tokens(subwords) --> IDs
- Decoding: IDs --> tokens --> text

### 4. Encoding pipeline
å½“è°ƒç”¨ Tokenizer.encode æˆ– encode_batch æ—¶ï¼Œè¾“å…¥çš„ text ç»è¿‡ä»¥ä¸‹æµç¨‹ï¼š
- normalization
- pre-tokenization
- model
- post-processig

1. å½’ä¸€åŒ– \
   ä½¿åŸå§‹çš„ string æ›´å°‘randomå’Œcleanerã€‚\
   å¸¸ç”¨æ“ä½œï¼š
   - å»ç©ºæ ¼: Strip()
   - å»è¯­æ°”è¯: StripAccents()
   - å…¨å°å†™: Lowercase()

    demo:
    ``` python
    from tokenizers import normalizers
    from tokenizers.normalizers import NFD, Lowercase, StripAccents

    tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])

    ```

2. é¢„åˆ†è¯ \
   å…ˆåˆæ­¥æŒ‰å•è¯åˆ†ï¼Œæœ€ç»ˆçš„åˆ†è¯ç»“æœå°±æ˜¯åŸºäºè¿™äº›å•è¯çš„ç»†åˆ†ã€‚\
   å¸¸ç”¨ï¼š
   - æŒ‰ç©ºæ ¼å’Œè¯­æ°”è¯ï¼šWhitespace()
   - æŒ‰æ•°å­—ï¼šDigits()
   è¾“å‡ºæ˜¯tupleå…ƒç»„çš„listï¼ŒåŒ…æ‹¬äº†å•è¯å’Œå®ƒåœ¨åŸå§‹å¥å­ä¸­è·¨è¶Šçš„ä½ç½®ã€‚

    demo:

    ``` python
    from tokenizers import pre_tokenizers
    from tokenizers.pre_tokenizers import Whitespace, Digits

    pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])
    pre_tokenizer.pre_tokenize_str("Call 911!")
    # [("Call", (0, 4)), ("9", (5, 6)), ("1", (6, 7)), ("1", (7, 8)), ("!", (8, 9))]
    ```

3. Model \
   modelè¿™éƒ¨åˆ†æ˜¯pipelineé‡Œéœ€è¦åœ¨corpusè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚è¾“å…¥çš„textç»è¿‡å½’ä¸€åŒ–å’Œé¢„åˆ†è¯åï¼Œmodelç”¨å­¦å¥½çš„è§„åˆ™æŠŠwordsåˆ†æˆtokensã€‚å¹¶åœ¨modelçš„vocabularyé‡ŒæŠŠtokenæ˜ å°„åˆ°å¯¹åº”idã€‚
   - BPE
   - Unigram
   - WordLevel
   - WordPiece

4. Post-Processing \
   åå¤„ç†æ˜¯åˆ†è¯æµç¨‹ä¸­çš„æœ€åä¸€æ­¥ï¼Œå¯¹Encodingæ‰§è¡Œæœ€åçš„å˜æ¢ï¼Œä¾‹å¦‚åŠ special tokensã€‚\
   - singleï¼šæŒ‡å®šå•å¥çš„æ¨¡ç‰ˆï¼Œ$A ä»£è¡¨å¥å­
   - pairï¼šæŒ‡å®š è¾“å…¥æ˜¯æˆå¯¹å¥å­ æƒ…å†µä¸‹çš„æ¨¡ç‰ˆï¼Œä¸€èˆ¬ç”¨ä¸åˆ°ã€‚$A ä»£è¡¨ç¬¬ä¸€å¥ï¼Œ$B ä»£è¡¨ç¬¬äºŒå¥ã€‚':1'ä»£è¡¨è¾“å…¥æ¯ä¸ªéƒ¨åˆ†çš„ type IDsï¼Œé»˜è®¤ä¸º :0ï¼Œæ‰€ä»¥ç¬¬ä¸€å¥å’Œç¬¬ä¸€ä¸ªåˆ†å‰²ç¬¦æ²¡æœ‰ç‰¹æŒ‡ã€‚ç¬¬äºŒå¥å’Œæœ€åçš„åˆ†å‰²ç¬¦è¢«è®¾ä¸º1.
   - special_tokensï¼šæŒ‡å®šç‰¹æ®Štokenså’Œè¯åº“ä¸­å¯¹åº”çš„idã€‚

   BERT demoï¼š
   ``` python
    from tokenizers.processors import TemplateProcessing

    tokenizer.post_processor = TemplateProcessing(
        single="[CLS] $A [SEP]",
        pair="[CLS] $A [SEP] $B:1 [SEP]:1",
        special_tokens=[
            ("[CLS]", tokenizer.token_to_id("[CLS]")),
            ("[SEP]", tokenizer.token_to_id("[SEP]")),
        ],
    )
   ```
   æ”¹å˜ä¸€ä¸ªåˆ†è¯å™¨çš„normalizeræˆ–pre-tokenizeråéƒ½éœ€è¦é‡æ–°è®­ç»ƒï¼Œä½†æ”¹å˜åå¤„ç†ä¸éœ€è¦ã€‚

   demoï¼šçœ‹ä¸‹åå¤„ç†æ•ˆæœ
   ``` python
   # è¾“å…¥ä¸ºå¥å­å¯¹
    output = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
    print(output.tokens)
    # ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]
    print(output.type_ids)
    # [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
   ```

### 5. Decoding pipeline
æŠŠmodelç”Ÿæˆçš„IDsè½¬å›textã€‚ä¼šå…ˆæŠŠIDsæ ¹æ®è¯åº“è½¬å›tokensï¼Œç„¶åå»é™¤æ‰€æœ‰special tokensï¼Œç„¶åæŠŠå‰©ä¸‹çš„tokensç”¨ç©ºæ ¼è¿èµ·æ¥ã€‚ \
å¦‚æœç”¨çš„åˆ†è¯modelé‡ŒåŠ äº†special charactersæ¥ä»£è¡¨subtokensï¼ˆä¾‹å¦‚BERTçš„WordPieceç”¨äº†'##'æ¥è¿æ¥å­è¯ï¼‰ï¼Œè§£ç æ—¶å°±éœ€è¦ç”¨å¯¹åº”è§„åˆ™çš„è§£ç å™¨ã€‚

``` python
output = bert_tokenizer.encode("Welcome to the ğŸ¤— Tokenizers library.")
print(output.tokens)
# ["[CLS]", "welcome", "to", "the", "[UNK]", "tok", "##eni", "##zer", "##s", "library", ".", "[SEP]"]
bert_tokenizer.decode(output.ids)
# "welcome to the tok ##eni ##zer ##s library ."


from tokenizers import decoders

bert_tokenizer.decoder = decoders.WordPiece()
bert_tokenizer.decode(output.ids)
# "welcome to the tokenizers library."

```

### 6. Using a pretrained tokenizer

``` python
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_file()

tokenizer = Tokenizer.from_pretrained("bert-base-uncased")

from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
```
