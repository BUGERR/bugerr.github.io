---
title: '反向传播计算'
date: 2025-05-14
permalink: /posts/2025/04/Transformer/
tags:
  - Deep Learning
  - 
  - 
---

# 反向传播计算

## SGD
1. 目的：优化损失函数，使网络输出逼近真实值。通过求损失函数对参数的导数，不断更新参数。
2. 过程：随机初始化参数，正向传播计算每个神经元的输出，然后反向传播从最后一层开始，从后向前，根据正向传播的结果，逐层计算每层权重的梯度，最后根据学习率更新参数。
3. 反向传播：根据链式法则从后向前逐层求导。假设 y = wx, L = 1/2*(y - gt)^2，求 L 对 w 的导数，要从后向前，先求 L 对 y 的导数 = y - gt，再求 y 对 w 的导数 = x。则 L 对 w 的梯度为 (y - gt) * x

<div style="text-align: center;">
  <img src="/images/bp_1.png" style="width: auto; height: auto;">
</div>

<div style="text-align: center;">
  <img src="/images/bp_2.png" style="width: auto; height: auto;">
</div>

<div style="text-align: center;">
  <img src="/images/bp_3.png" style="width: auto; height: auto;">
</div>

<div style="text-align: center;">
  <img src="/images/bp_4.png" style="width: auto; height: auto;">
</div>

<div style="text-align: center;">
  <img src="/images/bp_5.png" style="width: auto; height: auto;">
</div>

<div style="text-align: center;">
  <img src="/images/bp_6.png" style="width: auto; height: auto;">
</div>

## 极大似然估计