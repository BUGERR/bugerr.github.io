---
title: '论文阅读: GPT-2.0'
date: 2025-04-09
permalink: /posts/2025/04/GPT-2.0/
tags:
  - Deep Learning
  - GPT-2.0
  - LLMs
---

# 读 GPT-2.0 ｜ 语言模型是无监督的多任务学习

[TOC]

题目：Language Models are Unsupervised Multitask Learners

作者：

链接：

代码：

推荐阅读: 

<div style="text-align: center;">
  <img src="/images/GPT-2.0.png" style="width: auto; height: auto;">
</div>

## What:
1. 语言模型通过在 WebText 数据集（百万个网页）上训练，无需额外监督训练，就可以学到下游的自然语言处理任务。
2. 模型容量，对零样本迁移性能至关重要。GPT-2 是一个包含 1.5B 参数的 Transformer。
3. 针对多任务，输入有不同符号格式。
4. 改了 BPE，防止跨字符合并。

## Introduction
1. 任务特定（局限）：机器学习系统通过大数据集，大容量模型，监督训练，在训练任务表现出色。但系统**很脆弱，对数据分布和特定任务的微小变化很敏感**。当下的系统更像是狭隘的专家，而不是全能的通才。希望转向更通用的系统，可以执行很多任务。**最终无需为每个任务人工创建数据集并打标签**。
2. 过去训练**狭隘专家**的方法（局限）：收集训练数据集，样本要有任务的正确行为。然后训练模型来模拟这些行为，最后在独立同分布的保留样本上测试性能。但是，描述模型，阅读理解系统，图像分类器等针对多样输入通常有不规则行为，凸显了这种方法的缺点。
3. 推测：**单域数据集上的单任务训练导致模型缺少泛化**。因此评估要在多领域和多任务，benchmark 例如：GLUE，decaNLP（全部看成QA处理）。
4. **多任务学习（还是数据量问题）**：在 NLP 仍处于起步阶段，以（dataset，objective）pair 的形式。从元学习角度，每个对，都是从分布中采样的单个训练样本。当前的机器学习系统需要数千个样本来泛化函数，也即需要数千个多任务对。除了暴力扩大数据集和目标函数的规模，应另寻出路。
5. 当前语言任务方法：预训练+微调。发展的趋势表现为，**更灵活的迁移方式**。最初针对特定任务去学词向量，后来是迁移循环神经网络学到的的上下文表示，最近的研究表明，不需要任务特定的架构，**迁移自注意力的 blocks 就够了**。
6. 上述方法仍需要具体任务相关的监督学习。当没有监督数据时，另一项工作用语言建模（language model）执行特定任务，例如常识推理，情感分析。
7. 本文合并上述两条工作线，面向更通用的迁移方式。本文表明语言模型可以通过**零样本**执行下游任务，无需改任何参数和模型架构。

## Approach
### 概述
1. 核心是 language modeling。给定一组样本，每个样本是不同长度的序列，语言建模是对这组样本的无监督分布估计。由于语言天然有序，因此通常将联合概率按符号分解为条件概率的乘积。$$p(x) = \prod_{i=1}^{n} p(x_i \mid x_1, \cdots, x_{i-1})$$ 同样适用于序列对序列的条件概率。近年，可以计算这些条件概率的模型，表达能力有明显改进，例如自注意力架构的 Transformer。
2. 多任务处理方式（语言符号序列）：单任务可以表示为条件分布 p(output|input)，通用模型要执行多任务，因此对于相同的输入，还要以执行的任务为条件 p(output|input, task)。过去这种任务条件是在模型结构层面实现，例如任务特定的编码器解码器，算法层面例如 MAML 的内外循环优化。**但语言自身有一种更灵活的方式，将任务，输入，输出指定为符号的序列 sequence of symbols。**例如翻译：训练样本可以写为序列`(translate to french, english text, french text)`，阅读理解训练样本：`(answer the question, document, question, answer)`。此前的工作 MQAN 表明，训练单个模型，训练样本处理为此类格式，执行多任务是可行的。
3. 可行性：语言建模也可以学到这种格式，无需显式监督哪些符号是预测输出。有监督的目标与无监督目标相同，但有监督是仅在无监督序列的子集上进行评估，因此无监督目标的全局最小值也是监督目标的全局最小值。**通过这种简单设置，不用把密度估计作为原则上的训练目标，问题变为了优化无监督目标达到收敛。**初步实验表明，足够大的语言模型可以在这种简单设置下执行多任务学习，但学习速度比有监督慢很多。
4. 这种设计好的格式与原始自然语言有很大区别。因此有人认为，可以通过对话形式，通过教师输出的前向预测，学习没有奖励信号的QA任务，直接从未处理格式的自然语言中学习。但对话可能限制较大，**互联网有大量被动可用的信息，无需交互。本文推测是，当模型有了足够能力，为了更好预测，会开始去学推理和执行格式化序列中的任务，无关获取方式**。如果语言模型能够做到这一点，它实际上将执行无监督的多任务学习。我们通过分析语言模型在各种任务中零样本设置下的性能来测试是否如此。

### 2.1. Training dataset
1. 之前的工作都是在单个文本领域训练语言模型，例如新闻，百科，小说。我们的方法构建尽可能大和多样化的数据集，在不同领域和上下文中收集任务的自然语言表示。
2. 多样化且无限制的文本来源：网络抓取 Common Crawl，虽然数据量大，但质量低。
3. WebText 怎样生成：因此我们强调文本质量，只抓取了由人工过滤的网页。人工过滤很耗时，开始是**从 Reddit 爬取了所有出站链接**。看做一个启发式指标，**表示其他用户对该链接感兴趣，算是变相的人工筛选**。由此生成的数据集 WebText 包含这 4500 万个链接的文本子集。用 Dragnet 和 Newspaper 作为内容提取器从 HTML 中提取文本。本文所用 Webtext 初版，都是2017年12月之前的链接，删除了重复数据，和基于启发式的清理，得到800万个文档，总文本40GB。从中去除了所有 Wikipedia 的文档，因为是常见的数据集，还可能导致训练数据和测试任务重叠。

### 2.2. Input Representation
1. 通用语言模型应当能计算并生成任何字符串的概率。当前 LM 的预处理，例如小写，分词，词库外处理，都限制了可建模字符串的空间。而字节级别的分词又不如单词级别。
2. 字节对编码 BPE，作为 character 字符和单词 word 之间的语言建模，在频繁的单词级别和不频繁的字符级别符号序列之间做插值。最初 BPE 的实现并非对字节序列，而是对 Unicode 编码。这种实现需要包括 Unicode 符号的完整空间，以便对所有 Unicode 字符串进行建模。这就导致最基础的词汇表就 130,000，常用词库是 32,000 到 64,000 个token。相比之下，字节级别的 BPE 基础词汇表只要 256。然而，**直接在字节层面用 BPE 贪婪合并可能导致非最优合并，例如同一个词的许多版本：`dog. dog! dog? .`，导致有限词库和模型容量的非最优分配。为了避免这种情况，针对字节序列，防止 BPE 有任何跨字符类型的合并。**其中空格是例外，显著提高了压缩效率，同时在多个 token 中添加了最少的单词分片。
3. 这种输入表示结合了 word-level 的优点和 byte-level 的通用性。由于该方法可以为任何 Unicode string 分配一个概率，因此**可以在任何数据集上评价模型，无需考虑预处理，分词，词汇表。**

### 2.3. Model
1. 基于 GPT-1.0 改：
   - LayerNorm 移到了每层的输入，Pre-LN。
   - 在最终的自注意力块之后，加了额外的 LayerNorm。
   - 改了参数初始化，考虑了残差路径随着模型深度的积累：对于残差层的权重，初始化时除以根号 N，其中 N 是残差层的数量。
   - 词库从 40478 扩大到 50257
   - 上下文长度从 512 加到 1024
   - batchsize = 512
2. BPE：
   - GPT-2 的 BPE 不是像 GPT-1 那样基于 Unicode 字符进行分割，而是基于字节级别。
   - 这意味着 GPT-2 可以更加灵活地处理各种字符集和特殊符号，特别是非 ASCII 字符和表情符号等，这对于多语言支持和处理非英语文本非常有帮助。

## 4. Generalization vs Memorization
1. 视觉领域近期研究表明，常见图像数据集包含大量重复图像。例如：CIFAR-10 训练和测试有 3.3% 重叠。导致报道夸大了模型泛化性能。随着数据集增大，这个问题会变更明显，因此需要分析 WebText 训练集里有多少测试数据。