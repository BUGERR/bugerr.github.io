---
title: 'LLMs: Tokenlizer - BPE'
date: 2025-03-11
permalink: /posts/2025/03/BPE/
tags:
  - BPE
  - Tokenlizer
  - LLMs
---

# Byte-pair encoding (BPE)

[TOC]

## 1. æ¦‚è¿°
BPE æ˜¯ä¸€ç§ tokenize çš„æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹æ¥æ„å»ºæ›´å¤§çš„å­è¯å•å…ƒï¼Œä»è€Œå‡å°‘è¯æ±‡è¡¨çš„å¤§å°ï¼Œå¤„ç†ç¨€æœ‰è¯é—®é¢˜ã€‚å®ƒéœ€è¦å…ˆåœ¨ä¸€ä¸ªè¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°è¯è¡¨åæ‰èƒ½è¿›è¡Œç¼–ç å’Œè§£ç ã€‚

## 2. è®­ç»ƒæ­¥éª¤
1. ç¡®å®šè¶…å‚æ•°ï¼šè¯è¡¨å¤§å°ã€‚å¯¹æ–‡æœ¬æ ‡å‡†åŒ–ï¼Œé¢„åˆ†è¯ã€‚
2. åˆå§‹åŒ–åŸºç¡€è¯æ±‡è¡¨
   - æŠŠé¢„åˆ†è¯çš„ token æ‹†åˆ†ä¸ºå­—ç¬¦(character)åºåˆ—ï¼Œè¯æœ«ä½åŠ åç¼€'</w>'
3. ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹ bi-gram çš„é¢‘ç‡ã€‚ï¼ˆä¸ä¼šå‡ºç°ä»¥'</w>'å¼€å¤´çš„æƒ…å†µï¼‰
4. è¿­ä»£åˆå¹¶
   - åˆå¹¶æœ€é«˜é¢‘æ¬¡çš„å­—ç¬¦å¯¹
   - æ–°ç»„åˆåŠ å…¥è¯æ±‡è¡¨ encoder_vocabï¼ŒåŸæœ‰çš„ä¸åˆ 
   - åœ¨å¦ä¸€ä¸ªè¯è¡¨ merges ä¸­è®°å½•å­—ç¬¦å¯¹åˆå¹¶çš„å…ˆåé¡ºåºï¼Œä¾¿äºencoding
   - æŒ‰æ–°çš„è¯æ±‡è¡¨æ‹†åˆ†å•è¯ï¼Œé‡å¤2-4æ­¥éª¤ç›´åˆ°é¢„è®¾è¯æ±‡é‡

## GPT-1.0 ä¸­çš„ BPE å®ç°
ç”±äº Huggingface æä¾›çš„ BookCorpus æ•°æ®é›†å·²ç»ç»è¿‡äº†ç»†è‡´çš„åå¤„ç†ï¼Œå› æ­¤æˆ‘ä»¬æ— æ³•å®Œå…¨å¤ç°å‡ºåŸå§‹ GPT ä»£ç çš„ç»“æœã€‚ \
æœ¬æ–‡ä»…åŸºäºåŸå§‹å®ç° text_utils.py å®Œæˆäº†ç¼–ç å’Œè§£ç éƒ¨åˆ†çš„å·¥ä½œã€‚\
- encoder è¯è¡¨ï¼š`encoder_bpe_40000.json`
- è®°å½•å­—ç¬¦å¯¹åˆå¹¶å…ˆåé¡ºåºçš„è¯è¡¨ï¼š`vocab_40000.bpe`
BPE çš„è®­ç»ƒæµç¨‹å¯ä»¥å‚è€ƒæ–‡æœ« Karpathy çš„ minbpeã€‚

#### Training  
1. é¢„åˆ†è¯ï¼šç”¨ [`ftfy`](https://github.com/rspeer/python-ftfy) è§„èŒƒåŒ– Unicode å­—ç¬¦ï¼ŒæŠŠéæ ‡å‡†æ ‡ç‚¹ç»Ÿä¸€ï¼Œå¹¶æ›¿æ¢æ‰€æœ‰çš„ç©ºç™½å­—ç¬¦ä¸º `\n`ï¼Œç„¶åä½¿ç”¨ spacy çš„ [en_core_web_sm](https://spacy.io/models/en#en_core_web_sm) æ¨¡å‹è¿›è¡Œåˆ†è¯ï¼ˆè§ [bpe.py](./modules/bpe.py)ï¼‰ã€‚  
2. åˆå§‹åŒ–è¯æ±‡è¡¨ï¼šå°†æ•´ä¸ªæ–‡æœ¬è¯­æ–™åº“æ‹†åˆ†æˆå•å­—ç¬¦çš„å­è¯å•å…ƒï¼Œæœ€åä¸€ä¸ªå­—ç¬¦æ·»åŠ  `</w>`ã€‚åœ¨è®­ç»ƒåçš„è¯è¡¨ [encoder_bpe_40000.json](./datasets/bookcorpus/encoder_bpe_40000.json) ä¸­å¯ä»¥çœ‹å‡ºï¼Œid ä» 1-238 éƒ½ä¸ºå•ä¸ªå­—ç¬¦ï¼Œ239-476 ä¸ºå•ä¸ªå­—ç¬¦ + `</w>` çš„å½¢å¼ã€‚è¿™é‡Œçš„ `</w>` ä»£è¡¨ä¸€ä¸ª token çš„ç»“å°¾ã€‚ä¾‹å¦‚åœ¨å•è¯ `bamboo` ä¸­ï¼Œæœ€åä¸€ä¸ª `o` ä¼šè¢«è§†ä½œ `o</w>` ä»¥ä¸å€’æ•°ç¬¬äºŒä¸ª `o` åŒºåˆ†ã€‚  
3. ç»Ÿè®¡ bi-gram å­—ç¬¦å¯¹çš„é¢‘ç‡ã€‚get_stats æ¥æ”¶ä¸€ä¸ª listï¼Œéå†é‡Œé¢çš„æ¯ä¸ª pairï¼Œç»Ÿè®¡å‡ºç°æ¬¡æ•°ã€‚
4. åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹ï¼Œå¹¶å½¢æˆä¸€ä¸ªæ–°çš„å­è¯å•å…ƒã€‚æ›´æ–°è¯­æ–™åº“ä¸­çš„è¯æ±‡è¡¨ encoder_vocabï¼Œå¹¶åœ¨ merges ä¸­è®°å½•è¯¥åˆå¹¶æ“ä½œã€‚  
5. é‡å¤æ­¥éª¤ 3-4 40000 æ¬¡ï¼Œäºæ˜¯åœ¨ 476 ä¸ªå•ä¸ªè¯å…ƒçš„åŸºç¡€ä¸Šè·å¾—äº† 40000 ä¸ªæ–°çš„å­è¯å•å…ƒã€‚å†åŠ ä¸Š `<unk>` å’Œ `\n</w>` å…±è®¡ 40478 ä¸ªè¯å…ƒã€‚

#### minpe ä»£ç å®ç°
ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘æ¬¡ï¼š

``` python
def get_stats(ids, counts=None):
    """
    Given a list of integers, return a dictionary of counts of consecutive pairs
    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    æ³¨ï¼šæ²¡è€ƒè™‘ </w>
    Optionally allows to update an existing dictionary of counts
    """
    counts = {} if counts is None else counts
    for pair in zip(ids, ids[1:]): # iterate consecutive elements
        counts[pair] = counts.get(pair, 0) + 1
    return counts
```
1. ä¸ºä»€ä¹ˆæ˜¯å­—èŠ‚å¯¹ï¼Ÿå› ä¸ºç®—æ³•æœ€å¼€å§‹ç”¨çš„æ˜¯ utf-8 å¯¹ string ç¼–ç ï¼Œä¸€ä¸ªå­—èŠ‚8ä½æ•°ï¼Œèƒ½è¡¨ç¤º256ä¸ªå­—ç¬¦ã€‚å› æ­¤å­—ç¬¦åºåˆ—å…¶å®æ˜¯ä»¥ list of integers å­—èŠ‚åˆ—è¡¨çš„å½¢å¼è¡¨ç¤ºçš„ã€‚
``` python
token = 'T Ss'
text_bytes = token.encode("utf-8") # raw bytes
ids = list(text_bytes) # list of integers in range 0..255
print(ids)
# [84, 32, 83, 115] ç©ºæ ¼æ˜¯ç†Ÿæ‚‰çš„ 32
```
1. åˆå§‹åŒ–è¯å…¸ä¹Ÿåªæœ‰ 256 ä¸ªæ¡ç›®ã€‚ä¾‹å¦‚ï¼šè¾“å…¥"aaabdaaabac"ï¼Œåˆå¹¶ä¸‰æ¬¡ã€‚ç”±äºè¯è¡¨åˆå§‹åªæœ‰ 256 æ¡ï¼Œæ–°çš„å­è¯ç›´æ¥å¾€ååŠ ã€‚æ‰€ä»¥ `256: 'aa', 257: 'ab', 258: 'aaab'`ï¼Œç¼–ç è¾“å‡ºä¸ºï¼š[258, 100, 258, 97, 99]

``` python
vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes
```

3. ç‰¹æ®Štokenï¼š \
ç‰¹æ®Š token æ³¨å†Œä¸ºè¯è¡¨çš„æœ€åä¸€ä¸ªï¼Œç¼–å·å³è¯è¡¨å¤§å°ã€‚
``` python
from minbpe import RegexTokenizer
tokenizer = RegexTokenizer()
tokenizer.train(very_long_training_string, vocab_size=32768)
tokenizer.register_special_tokens({"<|endoftext|>": 32768})
tokenizer.encode("<|endoftext|>hello world", allowed_special="all")
```

#### Encoding  
0. åŠ è½½è®­ç»ƒå¥½çš„è¯è¡¨ mergesã€‚  
1. é¢„åˆ†è¯ï¼šå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œé¢„åˆ†è¯ï¼ŒåŒè®­ç»ƒé˜¶æ®µã€‚  
2. æŠŠé¢„åˆ†è¯çš„ token æ‹†åˆ†ä¸ºå­—ç¬¦(character)åºåˆ—ï¼Œæœ€åä¸€ä¸ªå­—ç¬¦æ·»åŠ  `</w>`ã€‚  
3. get_pairs ç»Ÿè®¡ bi-gram å­—ç¬¦å¯¹ã€‚
4. ä»è¿™äº› bi-gram å­—ç¬¦å¯¹ä¸­é€‰æ‹©åœ¨è¯è¡¨ merges ä¸­æœ€æ—©è¢«åˆå¹¶çš„ï¼Œè®°åš first å’Œ secondã€‚åœ¨åŸè¯ä¸­æ‰¾åˆ° first çš„ç´¢å¼•ï¼Œä¿ç•™ä¹‹å‰çš„éƒ¨åˆ†ã€‚å¦‚æœ first åå°±æ˜¯ secondï¼Œåˆå¹¶ï¼›å¦åˆ™ä¿ç•™è¿™ä¸ªå•ç‹¬çš„ firstã€‚åˆå¹¶ä¼šå½¢æˆä¸€ä¸ªæ–°çš„å­è¯ï¼Œç”¨è¿™ä¸ªæ–°çš„å­è¯æ›¿æ¢æ–‡æœ¬ä¸­çš„å­—ç¬¦ã€‚  
5. é‡å¤æ­¥éª¤ 3-4 ç›´åˆ°æ²¡æœ‰æ›´å¤šçš„æœ‰æ•ˆå­—ç¬¦å¯¹æˆ–è€…åªå‰©ä¸€ä¸ªå­—ç¬¦å•å…ƒã€‚  
6. ç¼“å­˜ç»“æœï¼Œå°†å­è¯å•å…ƒæ˜ å°„åˆ°è¯è¡¨ä¸­å¯¹åº” token çš„ idã€‚

#### Decoding  
0. åŠ è½½è®­ç»ƒå¥½çš„è¯è¡¨ encoder_vocabã€‚  
1. æ ¹æ®è¯è¡¨å»ºç«‹åå‘æ˜ å°„ï¼Œå°†ç»™å®š token id æ˜ å°„å›åŸå­è¯å³å¯ã€‚

### Code

encoderï¼šè¯è¡¨ï¼Œé”®å€¼å¯¹çš„å½¢å¼ã€‚

``` python
"""
bpe is short for Byte Pair Encoder. It translates arbitrary utf-8 strings into
sequences of integers, where each integer represents small chunks of commonly
occuring characters. This implementation is based on openai's gpt text_utils.py:
https://github.com/openai/finetune-transformer-lm/blob/master/text_utils.py
"""

import re
from typing import List, Optional, Union
import ftfy
import json
import spacy

from tqdm import tqdm


def get_pairs(word):
    """
    Return all bigrams as a set of tuples, of consecutive elements in the iterable word.
    
    ç»Ÿè®¡ç›¸é‚»å­—èŠ‚å¯¹ï¼Œdemoï¼š
    token = 'chat'
    word = tuple(token[:-1]) + (token[-1] + '</w>',)
    # ('c', 'h', 'a', 't</w>')
    pairs = get_pairs(word)
    # {('c', 'h'), ('h', 'a'), ('a', 't</w>')}

    bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float("inf")))
    first, second = bigram
    # bigram: a t</w>

    1:  ['c', 'h']
    2:  ['c', 'h', 'at</w>']
    pairs:  {('h', 'at</w>'), ('c', 'h')}

    # bigram: c h
    1:  []
    2:  ['ch']
    pairs:  {('ch', 'at</w>')}

    # bigram: ch at</w>
    1:  []
    2:  ['chat</w>']
    ('chat</w>',)
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs


def text_standardize(text):
    """
    fixes some issues the spacy tokenizer had on books corpus
    also does some whitespace standardization
    """
    text = text.replace("â€”", "-")
    text = text.replace("â€“", "-")
    text = text.replace("â€•", "-")
    text = text.replace("â€¦", "...")
    text = text.replace("Â´", "'")

    # add spaces around all punctuations (-, ~, !, ", ;, ?, +, `,`, ), (, \, /, *, [, ], }, {, |, _)
    # example: "Hi!Kami-chanw" will be converted to "Hi ! Kami - chanw"
    text = re.sub(
        r"""(-+|~+|!+|"+|;+|\?+|\++|,+|\)+|\(+|\\+|\/+|\*+|\[+|\]+|}+|{+|\|+|_+)""",
        r" \1 ",
        text,
    )

    # shrink spaces (or add spaces if not space exists) around `\n`
    # exmaple: "Hi\nKamichanw    \n" will be converted to "Hi \n Kamichanw \n "
    text = re.sub(r"\s*\n\s*", " \n ", text)

    # replace all space characters (e.g. `\t`) except `\n` with a single space
    # exmaple: "Hi\tKamichanw   \n" will be converted to "Hi Kamichanw \n "
    text = re.sub(r"[^\S\n]+", " ", text)
    return text.strip()


class BPETokenizer(object):
    """
    mostly a wrapper for a public python bpe tokenizer
    """

    def __init__(self, encoder_path, bpe_path):
        self.nlp = spacy.load(
            "en_core_web_sm",
            disable=["parser", "tagger", "ner", "textcat", "lemmatizer"],
        )
        self.encoder = json.load(open(encoder_path))
        self.decoder = {v: k for k, v in self.encoder.items()}
        merges = open(bpe_path).read().split("\n")[1:-1]
        merges = [tuple(merge.split()) for merge in merges]
        self.bpe_ranks = dict(zip(merges, range(len(merges))))
        self.cache = {}
        self.special_tokens = {"</w>"}

    def add_special_tokens(self, new_tokens: List[str]):
        start_idx = len(self.encoder)

        for i, token in enumerate(new_tokens):
            if token in self.encoder:
                raise ValueError(f"Token '{token}' already exists in the encoder.")

            self.encoder[token] = start_idx + i
            self.decoder[start_idx + i] = token

            # no need to update BPE ranks for special tokens as they are not merged
            self.cache[token] = token
        self.special_tokens.update(new_tokens)

    def get_vocab_size(self):
        return len(self.encoder)

    def bpe(self, token):
        word = tuple(token[:-1]) + (token[-1] + "</w>",)
        if token in self.cache:
            return self.cache[token]
        pairs = get_pairs(word)

        if not pairs:
            return token + "</w>"

        while True:

            # find the next lowest rank bigram that can be merged
            # the lower rank means earlier be merged
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
            if bigram not in self.bpe_ranks:
                break  # no more bigrams are eligible to be merged
            first, second = bigram

            # we will now replace all occurences of (first, second) in the list of current
            # words into one merged token first_second, in the output list new_words
            new_word = []
            i = 0
            while i < len(word):

                # find the next occurence of first in the sequence of current words
                # ä¿ç•™ first ä¹‹å‰çš„éƒ¨åˆ†
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except:
                    new_word.extend(word[i:])
                    break

                # if this occurence is also followed by second, then merge them into one
                # å¦‚æœ first åå°±æ˜¯ secondï¼Œåˆå¹¶ 
                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                # å¦åˆ™ï¼Œå’Œå…¶ä»–å­—ç¬¦ä¸€æ ·ï¼Œä¿ç•™è¿™ä¸ªå•ç‹¬çš„ first
                else:
                    new_word.append(word[i])
                    i += 1

            # all occurences of (first, second) have been merged to first_second
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)
        word = " ".join(word)
        if word == "\n  </w>":
            word = "\n</w>"
        self.cache[token] = word
        return word

    def token_to_id(self, token: str) -> int:
        return self.encoder.get(token, 0)

    def encode(
        self,
        texts: Union[str, List[str]],
        verbose: bool = True,
    ) -> List[List[int]]:
        if not isinstance(texts, list):
            texts = [texts]

        texts_tokens = []
        bar = tqdm(texts, ncols=80, leave=False) if verbose else texts

        for text in bar:
            text = self.nlp(text_standardize(ftfy.fix_text(text)))
            text_tokens = []
            for token in text:
                text_tokens.extend(
                    [
                        self.encoder.get(t, 0)
                        for t in self.bpe(token.text.lower()).split(" ")
                    ]
                )
            texts_tokens.append(text_tokens)
            
        return texts_tokens

    def decode(
        self, bpe_idx: Union[List[List[int]], List[int]], skip_special_tokens=True
    ):
        """lists of integers comes in, a list of string comes out"""
        if not isinstance(bpe_idx[0], list):
            bpe_idx = [bpe_idx]

        texts = []
        for idx in bpe_idx:
            # inverse map the integers to get the tokens
            tokens_merged = [self.decoder[token] for token in idx]
            text = "".join(tokens_merged)
            if skip_special_tokens:
                text = re.sub("|".join(self.special_tokens), " ", text)
            texts.append(text)

        return texts
```


## 3. ä¾‹å­: Train
corpus = 'chat chatt cat chap'
vocab_size = 8

### 1. é¦–è½®åˆå§‹åŒ–è¯è¡¨ï¼šæŒ‰å­—ç¬¦åˆ†è¯ï¼Œç»“æŸå­—ç¬¦åŠ åç¼€ç‰¹æ®Šæ ‡è¯†ï¼Œ
``` text
1. "chat"  â†’ ['c', 'h', 'a', 't</w>'] 
2. "chatt" â†’ ['c', 'h', 'a', 't', 't</w>'] 
3. "cat"   â†’ ['c', 'a', 't</w>'] 
4. "chap"  â†’ ['c', 'h', 'a', 'p</w>'] 
```

è¯æ±‡è¡¨ï¼š
``` text
{ 'c': 1, 'h': 2, 'a': 3, 't': 4, 'p</w>': 5, 't</w>: 6' } 
```

bpe è¡¨ï¼š
``` text

```

### 2. ç¬¬ä¸€æ¬¡åˆå¹¶
ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹é¢‘ç‡ï¼š
``` text
('c','h') â†’ 3æ¬¡ (æ¥è‡ª"chat", "chatt", "chap")
('h','a') â†’ 3æ¬¡ (åŒä¸Š)
('a','t</w>'') â†’ 2æ¬¡ ("chat", "cat")
('a','t) â†’ 1æ¬¡ ("chatt")
('t','t</w>') â†’ 1æ¬¡ ("chatt")
('c','a') â†’ 1æ¬¡ ("cat")
('a','p</w>') â†’ 1æ¬¡ ("chap")
```

æœ€é«˜é¢‘å¯¹ï¼š('c','h')ã€('h','a') å„3æ¬¡ï¼ˆä»»é€‰å…¶ä¸€ï¼Œå‡è®¾é€‰('c','h')ï¼‰

åˆå¹¶æ“ä½œï¼šå°†cå’Œhåˆå¹¶ä¸ºch

æ–°è¯æ±‡è¡¨ï¼š
``` text
{ 'c': 1, 'h': 2, 'a': 3, 't': 4, 'p</w>': 5, 't</w>: 6', 'ch': 7 } 
```

bpe è¡¨ï¼š
``` text
c h
```

è¯è¡¨æ›´æ–°åæ‹†åˆ†å•è¯ï¼š
``` text
chat  â†’ ['ch', 'a', 't</w>']
chatt â†’ ['ch', 'a', 't', 't'</w>']
cat   â†’ ['c', 'a', 't</w>'] (æœªå—å½±å“)
chap  â†’ ['ch', 'a', 'p</w>']
```

### 3. ç¬¬äºŒæ¬¡åˆå¹¶
ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹é¢‘ç‡ï¼š
``` text
('ch','a') â†’ 3æ¬¡ ("chat", "chatt", "chap")
('a','t</w>'') â†’ 2æ¬¡ ("chat", "cat")
('a','t) â†’ 1æ¬¡ ("chatt")
('t','t</w>') â†’ 1æ¬¡ ("chatt")
('c','a') â†’ 1æ¬¡ ("cat")
('a','p</w>') â†’ 1æ¬¡ ("chap")
```

æœ€é«˜é¢‘å¯¹ï¼š('ch','a')

åˆå¹¶æ“ä½œï¼šcha

æ–°è¯æ±‡è¡¨ï¼š
``` text
{ 'c': 1, 'h': 2, 'a': 3, 't': 4, 'p</w>': 5, 't</w>: 6', 'ch': 7, 'cha': 8 } 
```

bpe è¡¨ï¼š
``` text
c h
ch a
```

æ›´æ–°åçš„è¯æ‹†åˆ†ï¼š
``` text
chat  â†’ ['cha', 't</w>']
chatt â†’ ['cha', 't', 't</w>']
cat   â†’ ['c', 'a', 't</w>'] (æœªå—å½±å“)
chap  â†’ ['cha', 'p</w>']
```

### encoding
``` python
token = 'chat'
word = ('c', 'h', 'a', 't</w>')
pairs = get_pairs(word)
# {('c', 'h'), ('h', 'a'), ('a', 't</w>')}

bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float("inf")))
first, second = bigram
# bigram: a t</w>

1:  ['c', 'h']
2:  ['c', 'h', 'at</w>']
pairs:  {('h', 'at</w>'), ('c', 'h')}

# bigram: c h
1:  []
2:  ['ch']
pairs:  {('ch', 'at</w>')}

# bigram: ch at</w>
1:  []
2:  ['chat</w>']
('chat</w>',)
```

# minbpe
## 1. æ¦‚è¿°
1. æ˜¯å½“ä¸‹æœ€å¸¸ç”¨çš„å¤§æ¨¡å‹åˆ†è¯ç®—æ³•ï¼Œæ˜¯å­—èŠ‚å¯¹ï¼Œå› ä¸ºç”¨äº UTF-8 encoded stringsã€‚å›  GPT2 è€Œå¤§ç«ï¼Œå½“ä¸‹å¤§æ¨¡å‹éƒ½ç”¨BPEã€‚
2. Tokenizer çš„ä¸‰å¤§ä¸»è¦åŠŸèƒ½ï¼š
   - è®­ç»ƒè¯å…¸ï¼Œåˆå¹¶ pre-token
   - encode from text to tokens
   - decode from token to text

## 2. æ„å»ºè‡ªå·±çš„ GPT-4 Tokenizer
1. å†™ BasicTokenizer ç±»ï¼Œè¦æœ‰ä¸‰ä¸ªå…³é”®æ–¹æ³•ï¼Œtrainï¼Œencodeï¼Œdecodeï¼š
   - def train(self, text, vocab_size, verbose=False)
   - def encode(self, text)
   - def decode(self, ids)

2. æŠŠ Basic ç±» è½¬æˆ RegexTokenizer
3. load mergesï¼Œå¯¹æ¯”å’Œ tiktoken åº“çš„encode decodeæ•ˆæœ
``` python
# match this
import tiktoken
enc = tiktoken.get_encoding("cl100k_base") # this is the GPT-4 tokenizer
ids = enc.encode("hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ğŸ˜‰")
text = enc.decode(ids) # get the same text back
```
4. è¦æœ‰å¤„ç† special token çš„èƒ½åŠ›ï¼Œç±»æ¯”ï¼š
``` python
import tiktoken
enc = tiktoken.get_encoding("cl100k_base") # this is the GPT-4 tokenizer
ids = enc.encode("<|endoftext|>hello world", allowed_special="all")
```

5. Llama ç±»ç”¨çš„æ˜¯ sentencepiece åšåˆ†è¯ï¼ŒåŒºåˆ«åœ¨äºï¼Œ`sentencepiece runs BPE directly on Unicode code points instead of on UTF-8 encoded bytes`

## code
1. base.py

``` python
"""
Contains the base Tokenizer class and a few common helper functions.
The base class also contains the (common) save/load functionality.
It would be possible to be a lot more strict about the interface and
e.g. isolating all regex/pattern parts to the RegexTokenizer, but
some concessions are made for simplicity.
"""
import unicodedata

# -----------------------------------------------------------------------------
# a few helper functions useful for both BasicTokenizer and RegexTokenizer

def get_stats(ids, counts=None):
    """
    Given a list of integers, return a dictionary of counts of consecutive pairs
    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    Optionally allows to update an existing dictionary of counts
    """
    counts = {} if counts is None else counts
    for pair in zip(ids, ids[1:]): # iterate consecutive elements
        counts[pair] = counts.get(pair, 0) + 1
    return counts


def merge(ids, pair, idx):
    """
    In the list of integers (ids), replace all consecutive occurrences
    of pair with the new integer token idx
    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    """
    newids = []
    i = 0
    while i < len(ids):
        # if not at the very last position AND the pair matches, replace it
        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:
            newids.append(idx)
            i += 2
        else:
            newids.append(ids[i])
            i += 1
    return newids

# first two helper functions...
def replace_control_characters(s: str) -> str:
    # we don't want to print control characters
    # which distort the output (e.g. \n or much worse)
    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117
    # http://www.unicode.org/reports/tr44/#GC_Values_Table
    chars = []
    for ch in s:
        if unicodedata.category(ch)[0] != "C":
            chars.append(ch) # this character is ok
        else:
            chars.append(f"\\u{ord(ch):04x}") # escape
    return "".join(chars)

def render_token(t: bytes) -> str:
    # pretty print a token, escaping control characters
    s = t.decode('utf-8', errors='replace')
    s = replace_control_characters(s)
    return s

# -----------------------------------------------------------------------------
# the base Tokenizer class

class Tokenizer:
    """Base class for Tokenizers"""

    def __init__(self):
        # default: vocab size of 256 (all bytes), no merges, no patterns
        self.merges = {} # (int, int) -> int
        self.pattern = "" # str
        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}
        self.vocab = self._build_vocab() # int -> bytes

    def train(self, text, vocab_size, verbose=False):
        # Tokenizer can train a vocabulary of size vocab_size from text
        raise NotImplementedError

    def encode(self, text):
        # Tokenizer can encode a string into a list of integers
        raise NotImplementedError

    def decode(self, ids):
        # Tokenizer can decode a list of integers into a string
        raise NotImplementedError

    def _build_vocab(self):
        # vocab is simply and deterministically derived from merges
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def save(self, file_prefix):
        """
        Saves two files: file_prefix.vocab and file_prefix.model
        This is inspired (but not equivalent to!) sentencepiece's model saving:
        - model file is the critical one, intended for load()
        - vocab file is just a pretty printed version for human inspection only
        """
        # write the model: to be used in load() later
        model_file = file_prefix + ".model"
        with open(model_file, 'w') as f:
            # write the version, pattern and merges, that's all that's needed
            f.write("minbpe v1\n")
            f.write(f"{self.pattern}\n")
            # write the special tokens, first the number of them, then each one
            f.write(f"{len(self.special_tokens)}\n")
            for special, idx in self.special_tokens.items():
                f.write(f"{special} {idx}\n")
            # the merges dict
            for idx1, idx2 in self.merges:
                f.write(f"{idx1} {idx2}\n")
        # write the vocab: for the human to look at
        vocab_file = file_prefix + ".vocab"
        inverted_merges = {idx: pair for pair, idx in self.merges.items()}
        with open(vocab_file, "w", encoding="utf-8") as f:
            for idx, token in self.vocab.items():
                # note: many tokens may be partial utf-8 sequences
                # and cannot be decoded into valid strings. Here we're using
                # errors='replace' to replace them with the replacement char ï¿½.
                # this also means that we couldn't possibly use .vocab in load()
                # because decoding in this way is a lossy operation!
                s = render_token(token)
                # find the children of this token, if any
                if idx in inverted_merges:
                    # if this token has children, render it nicely as a merge
                    idx0, idx1 = inverted_merges[idx]
                    s0 = render_token(self.vocab[idx0])
                    s1 = render_token(self.vocab[idx1])
                    f.write(f"[{s0}][{s1}] -> [{s}] {idx}\n")
                else:
                    # otherwise this is leaf token, just print it
                    # (this should just be the first 256 tokens, the bytes)
                    f.write(f"[{s}] {idx}\n")

    def load(self, model_file):
        """Inverse of save() but only for the model file"""
        assert model_file.endswith(".model")
        # read the model file
        merges = {}
        special_tokens = {}
        idx = 256
        with open(model_file, 'r', encoding="utf-8") as f:
            # read the version
            version = f.readline().strip()
            assert version == "minbpe v1"
            # read the pattern
            self.pattern = f.readline().strip()
            # read the special tokens
            num_special = int(f.readline().strip())
            for _ in range(num_special):
                special, special_idx = f.readline().strip().split()
                special_tokens[special] = int(special_idx)
            # read the merges
            for line in f:
                idx1, idx2 = map(int, line.split())
                merges[(idx1, idx2)] = idx
                idx += 1
        self.merges = merges
        self.special_tokens = special_tokens
        self.vocab = self._build_vocab()
```

2. basic.py

``` python
"""
Minimal (byte-level) Byte Pair Encoding tokenizer.

Algorithmically follows along the GPT tokenizer:
https://github.com/openai/gpt-2/blob/master/src/encoder.py

But:
- Does not handle the regular expression splitting pattern.
- Does not handle any special tokens.
"""

from .base import Tokenizer, get_stats, merge


class BasicTokenizer(Tokenizer):

    def __init__(self):
        super().__init__()

    def train(self, text, vocab_size, verbose=False):
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # input text preprocessing
        text_bytes = text.encode("utf-8") # raw bytes
        ids = list(text_bytes) # list of integers in range 0..255

        # iteratively merge the most common pairs to create new tokens
        merges = {} # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes
        for i in range(num_merges):
            # count up the number of times every consecutive pair appears
            stats = get_stats(ids)
            # find the pair with the highest count
            pair = max(stats, key=stats.get)
            # mint a new token: assign it the next available id
            idx = 256 + i
            # replace all occurrences of pair in ids with idx
            ids = merge(ids, pair, idx)
            # save the merge
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]
            # prints
            if verbose:
                print(f"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences")

        # save class variables
        self.merges = merges # used in encode()
        self.vocab = vocab   # used in decode()

    def decode(self, ids):
        # given ids (list of integers), return Python string
        text_bytes = b"".join(self.vocab[idx] for idx in ids)
        text = text_bytes.decode("utf-8", errors="replace")
        return text

    def encode(self, text):
        # given a string text, return the token ids
        text_bytes = text.encode("utf-8") # raw bytes
        ids = list(text_bytes) # list of integers in range 0..255
        while len(ids) >= 2:
            # find the pair with the lowest merge index
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            # subtle: if there are no more merges available, the key will
            # result in an inf for every single pair, and the min will be
            # just the first pair in the list, arbitrarily
            # we can detect this terminating case by a membership check
            if pair not in self.merges:
                break # nothing else can be merged anymore
            # otherwise let's merge the best pair (lowest merge index)
            idx = self.merges[pair]
            ids = merge(ids, pair, idx)
        return ids
```

3. train.py

``` python
"""
Train our Tokenizers on some data, just to see them in action.
The whole thing runs in ~25 seconds on my laptop.
"""

import os
import time
from minbpe import BasicTokenizer, RegexTokenizer

# open some text and train a vocab of 512 tokens
text = open("tests/taylorswift.txt", "r", encoding="utf-8").read()

# create a directory for models, so we don't pollute the current directory
os.makedirs("models", exist_ok=True)

t0 = time.time()
for TokenizerClass, name in zip([BasicTokenizer, RegexTokenizer], ["basic", "regex"]):

    # construct the Tokenizer object and kick off verbose training
    tokenizer = TokenizerClass()
    tokenizer.train(text, 512, verbose=True)
    # writes two files in the models directory: name.model, and name.vocab
    prefix = os.path.join("models", name)
    tokenizer.save(prefix)
t1 = time.time()

print(f"Training took {t1 - t0:.2f} seconds")
```